from sklearn.feature_selection import mutual_info_classif
import csv
import numpy as np
import random
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC, LinearSVC
from sklearn.model_selection import KFold, cross_val_score
import csv
import copy
import math

class android(object):
	def __init__(self):
		self.split_ratio = 0.7
		ds = open('malware_per.csv')
		rdr = csv.reader(ds)
		self.data = list(rdr)
		self.data = random.sample(self.data, len(self.data))
		self.data = np.array(self.data)
		self.rows = len(self.data)
		self.columns = np.shape(self.data)[1]-1
		ds.close()

	def split_classLabel(self):
		cols = np.shape(self.data)[1]
		self.X = self.data[:,:cols-1]
		self.X = self.X.astype(np.float)
		self.y = self.data[:,cols-1]
		self.y = np.array(self.y)
		self.y = self.y.astype(np.int)
		self.y = np.ravel(self.y,order='C')
	'''
	def topFeatureList(self):

		features = [i.strip() for i in open("ben_per_features.txt").readlines()]
		features = np.array(features)
		mi = mutual_info_classif(self.X,self.y)
		self.featureind = sorted(range(len(mi)), key=lambda i: mi[i], reverse=True)[:25]
		top25 = features[self.featureind]
		print top25
		print mi[self.featureind]



	def splitDs(self):
		splitRows = int(self.split_ratio*len(self.data))
		trainData = self.X[:splitRows,self.featureind]
		trainTarget = self.y[:splitRows]
		testData = self.X[splitRows:,self.featureind]
		testTarget = self.y[splitRows:]
		return trainData, trainTarget, testData, testTarget		

	def Bayes(self):
		#training bayesian classifier
		clf = GaussianNB()
		trainData, trainTarget, testData, testTarget = self.splitDs()
		clf.fit(trainData,trainTarget)
		nbaccr = (clf.score(testData,testTarget))*100
		print nbaccr
	'''

	def SVM(self):
		clf = SVC()
		trainData, trainTarget, testData, testTarget = self.splitDs()
		clf.fit(trainData,trainTarget)
		svmaccr = (clf.score(testData,testTarget))*100
		print svmaccr

		model=SVC()
		model.fit(self.X,self.y)
		kfold = KFold(n_splits=10)
		accuracy_array=cross_val_score(model, self.X, self.y, cv=kfold, n_jobs=1)
		sum=0
		for value in accuracy_array:
			print value
			sum=sum+value


		accuracy=(float(sum) / len(accuracy_array))*100
		print "accuracy", accuracy
	'''	
	def separateClass(self):
		classes_tuples = {}
		for i in range(len(self.data)):
			vector = self.data[i]
			vector = map(int, vector)
			if (vector[-1] not in classes_tuples):
				classes_tuples[vector[-1]] = []
			classes_tuples[(vector[-1])].append(vector)
		return classes_tuples

	def freq(self):
		classes_tuples = self.separateClass()
		self.freqc = [0]*2
		for i in range(len(classes_tuples)): 
			self.freqc[i] = np.sum(classes_tuples[i],axis=0)
		return self.freqc,classes_tuples

	def mutual_info(self):
		frequency,classes_tuples = self.freq()
		frequency = np.array(frequency).T
		c0 = len(classes_tuples[0])
		c1 = len(classes_tuples[1])
		N = self.rows
		mi = [0]*self.columns
		for i in range(len(frequency)-1):

			nfc0 = frequency[i][0]
			nfc1 = frequency[i][1]
			if nfc0 == 0:
				nfc0 = 1
			if nfc1 == 0:
				nfc1 = 1
			diff0 = (c0-nfc0)
			diff1 = (c1-nfc1)
			if diff0 == 0:
				diff0 = 1
			if diff1 == 0:
				diff1 = 1
			
			mi[i] = (float(nfc0)/N)*math.log((float(N*nfc0)/((nfc0+nfc1)*(c0))),math.e) + \
			(float(nfc1)/N)*math.log((float(N*nfc1)/((nfc0+nfc1)*(c1))),math.e) + \
			(float(c0-nfc0)/N)*math.log((float(N*diff0)/((c0+c1-nfc0-nfc1)*(c0))),math.e) + \
			(float(c1-nfc1)/N)*math.log((float(N*diff1)/((c0+c1-nfc0-nfc1)*(c1))),math.e)

		return np.array(mi),frequency


	def topFeatureList2(self):
		features = [i.strip() for i in open("mal_per_features.txt").readlines()]
		features = np.array(features)
		mi,frequency = self.mutual_info()
		self.featureind = sorted(range(len(mi)), key=lambda i: mi[i], reverse=True)[:22]
		top25 = features[self.featureind]
		print mi[self.featureind]
		f_new=frequency[self.featureind]
		for i in range(0,len(top25)):
			print top25[i],mi[self.featureind[i]]



adr = android()
adr.split_classLabel()
adr.topFeatureList2()
#adr.Bayes()
#adr.SVM()
